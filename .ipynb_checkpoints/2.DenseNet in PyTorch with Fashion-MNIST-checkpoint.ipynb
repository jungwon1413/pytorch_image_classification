{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwserver/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "# used for logging to TensorBoard\n",
    "from tensorboard_logger import configure, log_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4*growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        nDenseBlocks = (depth-4) // 3\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2\n",
    "\n",
    "        nChannels = 2*growthRate\n",
    "        self.conv1 = nn.Conv2d(1, nChannels, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.fc = nn.Linear(nChannels, nClasses)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(Bottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            nChannels += growthRate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.dense3(out)\n",
    "        out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 7))\n",
    "        out = F.log_softmax(self.fc(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam = dict()\n",
    "\n",
    "# Dataset Configuration\n",
    "hparam['dataset'] = 'FashionMNIST'\t\t\t\t# Name of the dataset\n",
    "hparam['classes'] = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                     'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\t\t# List of classes\n",
    "hparam['data_split'] = 30000\t\t\t\t\t# Rearranging Train/Val Ratio (False if default)\n",
    "\n",
    "# Training Configuration\n",
    "hparam['print_freq'] = 10\t\t\t\t\t\t# Print frequency\n",
    "hparam['start_epoch'] = 0\t\t\t\t\t\t# Manual Epoch number (useful on restarts)\n",
    "hparam['resume'] = None\t\t\t\t\t\t\t# Path to latest checkpoint\n",
    "hparam['name'] = 'DenseNet_28_10'\t\t\t\t# Name of experiment\n",
    "hparam['tensorboard'] = False\t\t\t\t\t# Log progress to Tensorboard\n",
    "hparam['manual_seed'] = 40000\t\t\t\t\t# Seed Number\n",
    "\n",
    "# Hyperparameter Configuration\n",
    "hparam['epochs'] = 300\t\t\t\t\t\t\t# Number of total epochs to run\n",
    "hparam['batch_size'] = 128\t\t\t\t\t\t# Mini-batch size (256 recommended for 1080ti)\n",
    "hparam['lr'] = 0.1\t\t\t\t\t\t\t\t# Initial Learning Rate\n",
    "hparam['momentum'] = 0.9\t\t\t\t\t\t# Momentum\n",
    "hparam['nesterov'] = True\t\t\t\t\t\t# Nesterov momentum\n",
    "hparam['weight_decay'] = 1e-4\t\t\t\t\t# Weight-decay\n",
    "hparam['layers'] = 100\t\t\t\t\t\t\t# Total number of layers\n",
    "hparam['growthrate'] = 12\t\t\t\t\t\t# Growth Rate (DenseNet)\n",
    "hparam['reduction'] = 0.5\t\t\t\t\t\t# Reduction Rate (DenseNet)\n",
    "hparam['bottleneck'] = True\t\t\t\t\t\t# Bottleneck usage (DenseNet)\n",
    "hparam['augment'] = True\t\t\t\t\t\t# Whether to use standard augmentation\n",
    "\n",
    "# GPU Configuration\n",
    "hparam['cuda_device'] = '1'\t\t\t\t\t\t# Which GPU Devices to use (CPU: False)\n",
    "\n",
    "# Hardward Usage\n",
    "hparam['num_workers'] = 4\t\t\t\t\t\t# Number of Workers (1 for Single CUDA)\n",
    "\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(hparam):\n",
    "    global best_prec1\n",
    "    \n",
    "    if hparam['tensorboard']: configure(\"runs/%s\"%(hparam['name']))\n",
    "    \n",
    "    torch.manual_seed(hparam['manual_seed'])\n",
    "    print(\"Current CPU Random Seed: {}\".format(torch.initial_seed()))\n",
    "    \n",
    "    device_nums = str(hparam['cuda_device']).strip('[]')\n",
    "    device = torch.device(\"cuda:{}\".format(device_nums)\n",
    "                              if hparam['cuda_device'] else \"cpu\")\n",
    "    \n",
    "    if hparam['cuda_device']:\n",
    "        torch.cuda.manual_seed_all(hparam['manual_seed'])\n",
    "        print(\"Current GPU Random Seed: {}\".format(torch.cuda.initial_seed()))\n",
    "        \n",
    "        \n",
    "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                     std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "    if hparam['augment']:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: F.pad(x.unsqueeze(0),\n",
    "                                (4,4,4,4),mode='reflect').squeeze()),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomCrop(32),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ])\n",
    "    else:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "        ])\n",
    "\n",
    "    # Load Dataset\n",
    "    if hparam['data_split']:\n",
    "        # Concat and Split Dataset into new Train/Val ratio\n",
    "        (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "        N = hparam['data_split']\n",
    "        \n",
    "        test_data = np.concatenate((test_data, train_data[N:]), axis=0)\n",
    "        test_labels = np.concatenate((test_labels, train_labels[N:]), axis=0)\n",
    "        \n",
    "        train_data = train_data[:N]\n",
    "        train_labels = train_labels[:N]\n",
    "        \n",
    "        train_data = torch.from_numpy(train_data).float()\n",
    "        train_labels = torch.from_numpy(train_labels).long()\n",
    "        test_data = torch.from_numpy(test_data).float()\n",
    "        test_labels = torch.from_numpy(test_labels).long()\n",
    "        \n",
    "        print(\"Test Data:\", test_data.shape)\n",
    "        print(\"Train Data:\", train_data.shape)\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(train_data.unsqueeze(1), train_labels)\n",
    "        val_dataset = torch.utils.data.TensorDataset(test_data.unsqueeze(1), test_labels)\n",
    "\n",
    "        print(\"(Rearranged)Train: {}\\t Test: {}\".format(len(train_dataset), len(val_dataset)))\n",
    "        \n",
    "    else:\n",
    "        train_dataset = datasets.__dict__[hparam['dataset']](\n",
    "            './data', train=True, download=True, transform=transform_test)\n",
    "        val_dataset = datasets.__dict__[hparam['dataset']](\n",
    "            './data', train=False, transform=transform_test)\n",
    "    \n",
    "    kwargs = {'num_workers': hparam['num_workers'], 'pin_memory': True}\n",
    "    assert(hparam['dataset'] == 'FashionMNIST' or hparam['dataset'] == 'MNIST')\n",
    "    \n",
    "    # Import dataset to DataLoader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=hparam['batch_size'], shuffle=True, **kwargs)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=hparam['batch_size'], shuffle=True, **kwargs)\n",
    "    \n",
    "    # create model\n",
    "    model = DenseNet(growthRate=hparam['growthrate'], depth=hparam['layers'],\n",
    "                     reduction=hparam['reduction'], bottleneck=hparam['bottleneck'],\n",
    "                     nClasses=hparam['dataset'] == 'FashionMNIST' and len(hparam['classes']))\n",
    "\n",
    "    # for training on multiple GPUs.\n",
    "    # Use CUDA_VISIBLE_DEVICES=0,1 to specify which GPUs to use\n",
    "    # WARNING: Multi-GPU Mode is not working properly for now...\n",
    "    if type(hparam['cuda_device']) == list and len(hparam['cuda_device']) > 1:\n",
    "        print('\\n===> Training on Multi-GPU!')\n",
    "        model = nn.DataParallel(model, device_ids=hparam['cuda_device'], dim=0)\n",
    "    else:\n",
    "        print('\\n===> Training on Single-GPU!')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if hparam['resume']:\n",
    "        if os.path.isfile(hparam['resume']):\n",
    "            print(\"=> loading checkpoint '{}'\".format(hparam['resume']))\n",
    "            checkpoint = torch.load(hparam['resume'])\n",
    "            hparam['start_epoch'] = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(hparam['resume'], checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(hparam['resume']))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), hparam['lr'],\n",
    "                                momentum=hparam['momentum'],\n",
    "                                nesterov = hparam['nesterov'],\n",
    "                                weight_decay=hparam['weight_decay'])\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(hparam['start_epoch'], hparam['epochs']):\n",
    "        adjust_learning_rate(optimizer, epoch+1, hparam)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch, hparam)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion, epoch, hparam)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        }, is_best, hparam)\n",
    "    print('Best accuracy: ', best_prec1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    end_s = (end_time - start_time) % 60\n",
    "    end_h = (end_time - start_time) // 3600\n",
    "    end_m = (end_time - start_time - end_s) % 3600\n",
    "    print(\"[Total Time Elapsed: {:02d}h {:02d}m {:.2f}s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, hparam):\n",
    "    \"\"\"Train for one epoch on the training set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    \n",
    "    # Device Mode\n",
    "    device_nums = str(hparam['cuda_device']).strip('[]')\n",
    "    device = torch.device(\"cuda:{}\".format(device_nums)\n",
    "                              if hparam['cuda_device'] else \"cpu\")\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % hparam['print_freq'] == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      loss=losses, top1=top1))\n",
    "    # log to TensorBoard\n",
    "    if hparam['tensorboard']:\n",
    "        log_value('train_loss', losses.avg, epoch)\n",
    "        log_value('train_acc', top1.avg, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch, hparam):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    \n",
    "    # Device Mode\n",
    "    device_nums = str(hparam['cuda_device']).strip('[]')\n",
    "    device = torch.device(\"cuda:{}\".format(device_nums)\n",
    "                              if hparam['cuda_device'] else \"cpu\")\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % hparam['print_freq'] == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "    # log to TensorBoard\n",
    "    if hparam['tensorboard']:\n",
    "        log_value('val_loss', losses.avg, epoch)\n",
    "        log_value('val_acc', top1.avg, epoch)\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, hparam, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    directory = \"runs/{}/\".format(hparam['name'])\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'runs/%s/'%(hparam['name']) + 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, hparam):\n",
    "    \"\"\"Sets the learning rate to the initial LR divided by 10 at 150th and 225th epochs\"\"\"\n",
    "    learning_rate = hparam['lr'] * ((0.1 ** int(epoch >= 150)) * (0.1 ** int(epoch >= 225)))\n",
    "    # log to TensorBoard\n",
    "    if hparam['tensorboard']:\n",
    "        log_value('learning_rate', learning_rate, epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CPU Random Seed: 40000\n",
      "Current GPU Random Seed: 40000\n",
      "Test Data: torch.Size([40000, 28, 28])\n",
      "Train Data: torch.Size([30000, 28, 28])\n",
      "(Rearranged)Train: 30000\t Test: 40000\n",
      "\n",
      "===> Training on Single-GPU!\n",
      "Number of model parameters: 768730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwserver/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/235]\tTime 1.813 (1.813)\tLoss 2.3503 (2.3503)\tPrec@1 6.250 (6.250)\n",
      "Epoch: [0][10/235]\tTime 0.142 (0.302)\tLoss 1.4891 (1.9845)\tPrec@1 50.781 (28.693)\n",
      "Epoch: [0][20/235]\tTime 0.143 (0.226)\tLoss 1.1294 (1.6423)\tPrec@1 62.500 (41.592)\n",
      "Epoch: [0][30/235]\tTime 0.144 (0.199)\tLoss 0.9251 (1.4187)\tPrec@1 72.656 (49.294)\n",
      "Epoch: [0][40/235]\tTime 0.143 (0.186)\tLoss 0.8480 (1.2566)\tPrec@1 71.875 (55.202)\n",
      "Epoch: [0][50/235]\tTime 0.144 (0.177)\tLoss 0.6458 (1.1446)\tPrec@1 75.781 (59.099)\n",
      "Epoch: [0][60/235]\tTime 0.143 (0.172)\tLoss 0.7227 (1.0729)\tPrec@1 69.531 (61.335)\n",
      "Epoch: [0][70/235]\tTime 0.143 (0.168)\tLoss 0.7406 (1.0094)\tPrec@1 70.312 (63.567)\n",
      "Epoch: [0][80/235]\tTime 0.143 (0.165)\tLoss 0.5143 (0.9623)\tPrec@1 80.469 (65.191)\n",
      "Epoch: [0][90/235]\tTime 0.143 (0.162)\tLoss 0.5522 (0.9158)\tPrec@1 78.125 (66.784)\n",
      "Epoch: [0][100/235]\tTime 0.143 (0.160)\tLoss 0.5178 (0.8808)\tPrec@1 78.125 (68.015)\n",
      "Epoch: [0][110/235]\tTime 0.143 (0.159)\tLoss 0.5580 (0.8488)\tPrec@1 80.469 (69.123)\n",
      "Epoch: [0][120/235]\tTime 0.144 (0.157)\tLoss 0.4764 (0.8192)\tPrec@1 80.469 (70.138)\n",
      "Epoch: [0][130/235]\tTime 0.143 (0.156)\tLoss 0.4355 (0.7930)\tPrec@1 82.031 (71.046)\n",
      "Epoch: [0][140/235]\tTime 0.143 (0.155)\tLoss 0.3739 (0.7701)\tPrec@1 86.719 (71.864)\n",
      "Epoch: [0][150/235]\tTime 0.143 (0.155)\tLoss 0.5221 (0.7504)\tPrec@1 83.594 (72.599)\n",
      "Epoch: [0][160/235]\tTime 0.143 (0.154)\tLoss 0.4865 (0.7316)\tPrec@1 81.250 (73.287)\n",
      "Epoch: [0][170/235]\tTime 0.143 (0.153)\tLoss 0.2972 (0.7140)\tPrec@1 88.281 (73.835)\n",
      "Epoch: [0][180/235]\tTime 0.143 (0.153)\tLoss 0.3656 (0.6980)\tPrec@1 89.844 (74.452)\n",
      "Epoch: [0][190/235]\tTime 0.143 (0.152)\tLoss 0.4330 (0.6817)\tPrec@1 86.719 (75.053)\n",
      "Epoch: [0][200/235]\tTime 0.143 (0.152)\tLoss 0.3632 (0.6662)\tPrec@1 87.500 (75.587)\n",
      "Epoch: [0][210/235]\tTime 0.143 (0.151)\tLoss 0.4260 (0.6554)\tPrec@1 85.938 (75.977)\n",
      "Epoch: [0][220/235]\tTime 0.143 (0.151)\tLoss 0.3882 (0.6437)\tPrec@1 87.500 (76.421)\n",
      "Epoch: [0][230/235]\tTime 0.143 (0.151)\tLoss 0.2783 (0.6314)\tPrec@1 91.406 (76.880)\n",
      "Test: [0/313]\tTime 0.216 (0.216)\tLoss 0.5337 (0.5337)\tPrec@1 77.344 (77.344)\n",
      "Test: [10/313]\tTime 0.041 (0.057)\tLoss 0.4688 (0.5060)\tPrec@1 82.031 (81.037)\n",
      "Test: [20/313]\tTime 0.041 (0.050)\tLoss 0.6193 (0.4829)\tPrec@1 76.562 (81.808)\n",
      "Test: [30/313]\tTime 0.041 (0.047)\tLoss 0.4692 (0.4709)\tPrec@1 82.812 (82.132)\n",
      "Test: [40/313]\tTime 0.041 (0.045)\tLoss 0.3969 (0.4646)\tPrec@1 82.031 (82.355)\n",
      "Test: [50/313]\tTime 0.041 (0.045)\tLoss 0.5503 (0.4695)\tPrec@1 79.688 (82.184)\n",
      "Test: [60/313]\tTime 0.041 (0.044)\tLoss 0.3930 (0.4696)\tPrec@1 85.156 (82.185)\n",
      "Test: [70/313]\tTime 0.041 (0.044)\tLoss 0.4630 (0.4709)\tPrec@1 79.688 (82.251)\n",
      "Test: [80/313]\tTime 0.041 (0.043)\tLoss 0.3967 (0.4703)\tPrec@1 84.375 (82.263)\n",
      "Test: [90/313]\tTime 0.041 (0.043)\tLoss 0.3896 (0.4674)\tPrec@1 83.594 (82.315)\n",
      "Test: [100/313]\tTime 0.041 (0.043)\tLoss 0.4826 (0.4646)\tPrec@1 78.906 (82.302)\n",
      "Test: [110/313]\tTime 0.041 (0.043)\tLoss 0.5551 (0.4642)\tPrec@1 77.344 (82.299)\n",
      "Test: [120/313]\tTime 0.041 (0.043)\tLoss 0.4167 (0.4628)\tPrec@1 88.281 (82.412)\n",
      "Test: [130/313]\tTime 0.041 (0.043)\tLoss 0.3593 (0.4620)\tPrec@1 87.500 (82.467)\n",
      "Test: [140/313]\tTime 0.041 (0.042)\tLoss 0.6412 (0.4661)\tPrec@1 78.125 (82.303)\n",
      "Test: [150/313]\tTime 0.041 (0.042)\tLoss 0.7153 (0.4707)\tPrec@1 75.781 (82.186)\n",
      "Test: [160/313]\tTime 0.041 (0.042)\tLoss 0.4835 (0.4697)\tPrec@1 80.469 (82.250)\n",
      "Test: [170/313]\tTime 0.041 (0.042)\tLoss 0.6230 (0.4708)\tPrec@1 77.344 (82.255)\n",
      "Test: [180/313]\tTime 0.041 (0.042)\tLoss 0.5964 (0.4710)\tPrec@1 80.469 (82.282)\n",
      "Test: [190/313]\tTime 0.041 (0.042)\tLoss 0.3660 (0.4712)\tPrec@1 89.844 (82.305)\n",
      "Test: [200/313]\tTime 0.041 (0.042)\tLoss 0.5708 (0.4715)\tPrec@1 72.656 (82.226)\n",
      "Test: [210/313]\tTime 0.041 (0.042)\tLoss 0.5075 (0.4722)\tPrec@1 79.688 (82.153)\n",
      "Test: [220/313]\tTime 0.041 (0.042)\tLoss 0.4299 (0.4716)\tPrec@1 82.812 (82.134)\n",
      "Test: [230/313]\tTime 0.041 (0.042)\tLoss 0.4081 (0.4717)\tPrec@1 85.156 (82.126)\n",
      "Test: [240/313]\tTime 0.041 (0.042)\tLoss 0.4889 (0.4697)\tPrec@1 82.031 (82.210)\n",
      "Test: [250/313]\tTime 0.041 (0.042)\tLoss 0.4513 (0.4684)\tPrec@1 81.250 (82.286)\n",
      "Test: [260/313]\tTime 0.041 (0.042)\tLoss 0.4320 (0.4682)\tPrec@1 80.469 (82.340)\n",
      "Test: [270/313]\tTime 0.041 (0.042)\tLoss 0.4008 (0.4671)\tPrec@1 85.156 (82.397)\n",
      "Test: [280/313]\tTime 0.041 (0.042)\tLoss 0.3977 (0.4666)\tPrec@1 82.812 (82.409)\n",
      "Test: [290/313]\tTime 0.041 (0.042)\tLoss 0.3682 (0.4666)\tPrec@1 83.594 (82.394)\n",
      "Test: [300/313]\tTime 0.041 (0.042)\tLoss 0.3993 (0.4665)\tPrec@1 86.719 (82.382)\n",
      "Test: [310/313]\tTime 0.041 (0.042)\tLoss 0.4116 (0.4659)\tPrec@1 82.031 (82.398)\n",
      " * Prec@1 82.397\n",
      "Epoch: [1][0/235]\tTime 0.299 (0.299)\tLoss 0.3306 (0.3306)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [1][10/235]\tTime 0.143 (0.157)\tLoss 0.3109 (0.3653)\tPrec@1 90.625 (86.932)\n",
      "Epoch: [1][20/235]\tTime 0.144 (0.151)\tLoss 0.4228 (0.3537)\tPrec@1 85.938 (87.723)\n",
      "Epoch: [1][30/235]\tTime 0.143 (0.148)\tLoss 0.3586 (0.3516)\tPrec@1 86.719 (87.324)\n",
      "Epoch: [1][40/235]\tTime 0.143 (0.147)\tLoss 0.2962 (0.3445)\tPrec@1 89.062 (87.309)\n",
      "Epoch: [1][50/235]\tTime 0.144 (0.146)\tLoss 0.2938 (0.3420)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [1][60/235]\tTime 0.143 (0.146)\tLoss 0.4230 (0.3443)\tPrec@1 85.156 (87.474)\n",
      "Epoch: [1][70/235]\tTime 0.144 (0.146)\tLoss 0.3218 (0.3430)\tPrec@1 85.938 (87.588)\n",
      "Epoch: [1][80/235]\tTime 0.144 (0.145)\tLoss 0.3904 (0.3421)\tPrec@1 83.594 (87.674)\n",
      "Epoch: [1][90/235]\tTime 0.143 (0.145)\tLoss 0.2438 (0.3400)\tPrec@1 92.969 (87.766)\n",
      "Epoch: [1][100/235]\tTime 0.143 (0.145)\tLoss 0.3078 (0.3356)\tPrec@1 89.062 (87.949)\n",
      "Epoch: [1][110/235]\tTime 0.143 (0.145)\tLoss 0.4213 (0.3348)\tPrec@1 85.156 (88.000)\n",
      "Epoch: [1][120/235]\tTime 0.143 (0.145)\tLoss 0.2347 (0.3341)\tPrec@1 93.750 (87.984)\n",
      "Epoch: [1][130/235]\tTime 0.144 (0.145)\tLoss 0.2711 (0.3343)\tPrec@1 90.625 (87.971)\n",
      "Epoch: [1][140/235]\tTime 0.144 (0.145)\tLoss 0.2989 (0.3371)\tPrec@1 89.844 (87.888)\n",
      "Epoch: [1][150/235]\tTime 0.143 (0.144)\tLoss 0.3295 (0.3348)\tPrec@1 86.719 (87.997)\n",
      "Epoch: [1][160/235]\tTime 0.143 (0.144)\tLoss 0.3583 (0.3328)\tPrec@1 87.500 (88.043)\n",
      "Epoch: [1][170/235]\tTime 0.143 (0.144)\tLoss 0.4577 (0.3328)\tPrec@1 85.156 (88.030)\n",
      "Epoch: [1][180/235]\tTime 0.144 (0.144)\tLoss 0.3144 (0.3293)\tPrec@1 89.062 (88.147)\n",
      "Epoch: [1][190/235]\tTime 0.144 (0.144)\tLoss 0.2729 (0.3280)\tPrec@1 90.625 (88.195)\n",
      "Epoch: [1][200/235]\tTime 0.144 (0.144)\tLoss 0.3885 (0.3280)\tPrec@1 85.156 (88.204)\n",
      "Epoch: [1][210/235]\tTime 0.145 (0.144)\tLoss 0.4226 (0.3289)\tPrec@1 85.156 (88.133)\n",
      "Epoch: [1][220/235]\tTime 0.144 (0.144)\tLoss 0.2317 (0.3285)\tPrec@1 92.969 (88.122)\n",
      "Epoch: [1][230/235]\tTime 0.144 (0.144)\tLoss 0.3370 (0.3268)\tPrec@1 89.844 (88.200)\n",
      "Test: [0/313]\tTime 0.210 (0.210)\tLoss 0.4514 (0.4514)\tPrec@1 82.031 (82.031)\n",
      "Test: [10/313]\tTime 0.041 (0.057)\tLoss 0.3456 (0.3685)\tPrec@1 90.625 (86.790)\n",
      "Test: [20/313]\tTime 0.041 (0.049)\tLoss 0.4539 (0.3848)\tPrec@1 84.375 (86.272)\n",
      "Test: [30/313]\tTime 0.041 (0.047)\tLoss 0.4777 (0.3989)\tPrec@1 81.250 (85.938)\n",
      "Test: [40/313]\tTime 0.041 (0.045)\tLoss 0.3843 (0.3963)\tPrec@1 86.719 (86.338)\n",
      "Test: [50/313]\tTime 0.041 (0.045)\tLoss 0.4498 (0.4101)\tPrec@1 82.031 (85.769)\n",
      "Test: [60/313]\tTime 0.041 (0.044)\tLoss 0.3867 (0.4067)\tPrec@1 88.281 (86.002)\n",
      "Test: [70/313]\tTime 0.041 (0.044)\tLoss 0.4001 (0.4090)\tPrec@1 85.938 (85.993)\n",
      "Test: [80/313]\tTime 0.041 (0.043)\tLoss 0.3426 (0.4103)\tPrec@1 88.281 (85.928)\n",
      "Test: [90/313]\tTime 0.041 (0.043)\tLoss 0.4542 (0.4102)\tPrec@1 86.719 (85.963)\n",
      "Test: [100/313]\tTime 0.041 (0.043)\tLoss 0.4090 (0.4136)\tPrec@1 86.719 (85.922)\n",
      "Test: [110/313]\tTime 0.041 (0.043)\tLoss 0.2927 (0.4108)\tPrec@1 89.844 (86.071)\n",
      "Test: [120/313]\tTime 0.041 (0.043)\tLoss 0.4291 (0.4107)\tPrec@1 86.719 (86.099)\n",
      "Test: [130/313]\tTime 0.041 (0.043)\tLoss 0.4701 (0.4112)\tPrec@1 83.594 (85.997)\n",
      "Test: [140/313]\tTime 0.041 (0.043)\tLoss 0.4892 (0.4137)\tPrec@1 86.719 (85.910)\n",
      "Test: [150/313]\tTime 0.041 (0.043)\tLoss 0.4704 (0.4168)\tPrec@1 84.375 (85.813)\n",
      "Test: [160/313]\tTime 0.041 (0.042)\tLoss 0.5673 (0.4191)\tPrec@1 78.906 (85.739)\n",
      "Test: [170/313]\tTime 0.041 (0.042)\tLoss 0.4283 (0.4167)\tPrec@1 86.719 (85.800)\n",
      "Test: [180/313]\tTime 0.041 (0.042)\tLoss 0.2458 (0.4184)\tPrec@1 93.750 (85.743)\n",
      "Test: [190/313]\tTime 0.041 (0.042)\tLoss 0.4182 (0.4160)\tPrec@1 86.719 (85.798)\n",
      "Test: [200/313]\tTime 0.041 (0.042)\tLoss 0.4650 (0.4179)\tPrec@1 81.250 (85.751)\n",
      "Test: [210/313]\tTime 0.041 (0.042)\tLoss 0.3561 (0.4177)\tPrec@1 89.844 (85.726)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [220/313]\tTime 0.041 (0.042)\tLoss 0.3537 (0.4178)\tPrec@1 92.188 (85.701)\n",
      "Test: [230/313]\tTime 0.041 (0.042)\tLoss 0.2963 (0.4169)\tPrec@1 88.281 (85.697)\n",
      "Test: [240/313]\tTime 0.041 (0.042)\tLoss 0.5729 (0.4175)\tPrec@1 82.812 (85.662)\n",
      "Test: [250/313]\tTime 0.041 (0.042)\tLoss 0.3128 (0.4170)\tPrec@1 92.969 (85.701)\n",
      "Test: [260/313]\tTime 0.041 (0.042)\tLoss 0.3435 (0.4170)\tPrec@1 88.281 (85.698)\n",
      "Test: [270/313]\tTime 0.041 (0.042)\tLoss 0.2828 (0.4177)\tPrec@1 91.406 (85.669)\n",
      "Test: [280/313]\tTime 0.041 (0.042)\tLoss 0.4302 (0.4175)\tPrec@1 84.375 (85.654)\n",
      "Test: [290/313]\tTime 0.041 (0.042)\tLoss 0.4773 (0.4172)\tPrec@1 86.719 (85.701)\n",
      "Test: [300/313]\tTime 0.041 (0.042)\tLoss 0.4150 (0.4179)\tPrec@1 83.594 (85.660)\n",
      "Test: [310/313]\tTime 0.041 (0.042)\tLoss 0.4680 (0.4186)\tPrec@1 86.719 (85.666)\n",
      " * Prec@1 85.657\n",
      "Epoch: [2][0/235]\tTime 0.300 (0.300)\tLoss 0.2812 (0.2812)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [2][10/235]\tTime 0.144 (0.158)\tLoss 0.2099 (0.2343)\tPrec@1 92.969 (91.477)\n",
      "Epoch: [2][20/235]\tTime 0.144 (0.151)\tLoss 0.2123 (0.2515)\tPrec@1 92.969 (91.034)\n",
      "Epoch: [2][30/235]\tTime 0.144 (0.149)\tLoss 0.2094 (0.2688)\tPrec@1 90.625 (90.222)\n",
      "Epoch: [2][40/235]\tTime 0.144 (0.148)\tLoss 0.3290 (0.2664)\tPrec@1 85.156 (90.206)\n",
      "Epoch: [2][50/235]\tTime 0.144 (0.147)\tLoss 0.2499 (0.2712)\tPrec@1 89.844 (89.920)\n",
      "Epoch: [2][60/235]\tTime 0.144 (0.147)\tLoss 0.3118 (0.2726)\tPrec@1 89.844 (89.933)\n",
      "Epoch: [2][70/235]\tTime 0.144 (0.146)\tLoss 0.2455 (0.2751)\tPrec@1 92.188 (89.921)\n",
      "Epoch: [2][80/235]\tTime 0.144 (0.146)\tLoss 0.2230 (0.2741)\tPrec@1 94.531 (90.095)\n",
      "Epoch: [2][90/235]\tTime 0.144 (0.146)\tLoss 0.2178 (0.2708)\tPrec@1 93.750 (90.230)\n",
      "Epoch: [2][100/235]\tTime 0.146 (0.146)\tLoss 0.2813 (0.2715)\tPrec@1 92.188 (90.207)\n",
      "Epoch: [2][110/235]\tTime 0.144 (0.146)\tLoss 0.2133 (0.2705)\tPrec@1 91.406 (90.252)\n",
      "Epoch: [2][120/235]\tTime 0.144 (0.145)\tLoss 0.2265 (0.2693)\tPrec@1 92.969 (90.251)\n",
      "Epoch: [2][130/235]\tTime 0.144 (0.145)\tLoss 0.1609 (0.2692)\tPrec@1 93.750 (90.261)\n",
      "Epoch: [2][140/235]\tTime 0.143 (0.145)\tLoss 0.1815 (0.2692)\tPrec@1 92.969 (90.248)\n",
      "Epoch: [2][150/235]\tTime 0.144 (0.145)\tLoss 0.3251 (0.2688)\tPrec@1 86.719 (90.294)\n",
      "Epoch: [2][160/235]\tTime 0.143 (0.145)\tLoss 0.1653 (0.2687)\tPrec@1 96.094 (90.334)\n",
      "Epoch: [2][170/235]\tTime 0.144 (0.145)\tLoss 0.2939 (0.2698)\tPrec@1 86.719 (90.301)\n",
      "Epoch: [2][180/235]\tTime 0.144 (0.145)\tLoss 0.2833 (0.2690)\tPrec@1 88.281 (90.319)\n",
      "Epoch: [2][190/235]\tTime 0.144 (0.145)\tLoss 0.1872 (0.2686)\tPrec@1 92.969 (90.330)\n",
      "Epoch: [2][200/235]\tTime 0.144 (0.145)\tLoss 0.2411 (0.2672)\tPrec@1 89.844 (90.392)\n",
      "Epoch: [2][210/235]\tTime 0.144 (0.145)\tLoss 0.3032 (0.2679)\tPrec@1 91.406 (90.366)\n",
      "Epoch: [2][220/235]\tTime 0.144 (0.145)\tLoss 0.2348 (0.2664)\tPrec@1 88.281 (90.356)\n",
      "Epoch: [2][230/235]\tTime 0.144 (0.145)\tLoss 0.2506 (0.2650)\tPrec@1 89.844 (90.395)\n",
      "Test: [0/313]\tTime 0.199 (0.199)\tLoss 0.4116 (0.4116)\tPrec@1 82.812 (82.812)\n",
      "Test: [10/313]\tTime 0.041 (0.056)\tLoss 0.4354 (0.4324)\tPrec@1 83.594 (84.233)\n",
      "Test: [20/313]\tTime 0.041 (0.049)\tLoss 0.3602 (0.4144)\tPrec@1 88.281 (85.454)\n",
      "Test: [30/313]\tTime 0.041 (0.046)\tLoss 0.4583 (0.4001)\tPrec@1 87.500 (86.013)\n",
      "Test: [40/313]\tTime 0.041 (0.045)\tLoss 0.3768 (0.4000)\tPrec@1 86.719 (85.861)\n",
      "Test: [50/313]\tTime 0.041 (0.044)\tLoss 0.3812 (0.3947)\tPrec@1 89.062 (86.014)\n",
      "Test: [60/313]\tTime 0.041 (0.044)\tLoss 0.4265 (0.3954)\tPrec@1 85.156 (85.886)\n",
      "Test: [70/313]\tTime 0.041 (0.044)\tLoss 0.4930 (0.3962)\tPrec@1 83.594 (85.893)\n",
      "Test: [80/313]\tTime 0.041 (0.043)\tLoss 0.5858 (0.3974)\tPrec@1 81.250 (85.928)\n",
      "Test: [90/313]\tTime 0.041 (0.043)\tLoss 0.4451 (0.4006)\tPrec@1 85.156 (85.860)\n",
      "Test: [100/313]\tTime 0.041 (0.043)\tLoss 0.3928 (0.3991)\tPrec@1 83.594 (85.791)\n",
      "Test: [110/313]\tTime 0.041 (0.043)\tLoss 0.3326 (0.3976)\tPrec@1 84.375 (85.740)\n",
      "Test: [120/313]\tTime 0.041 (0.043)\tLoss 0.3747 (0.4006)\tPrec@1 88.281 (85.718)\n",
      "Test: [130/313]\tTime 0.041 (0.043)\tLoss 0.2885 (0.3970)\tPrec@1 90.625 (85.812)\n",
      "Test: [140/313]\tTime 0.041 (0.042)\tLoss 0.3736 (0.3946)\tPrec@1 85.156 (85.871)\n",
      "Test: [150/313]\tTime 0.041 (0.042)\tLoss 0.2909 (0.3935)\tPrec@1 92.188 (85.875)\n",
      "Test: [160/313]\tTime 0.041 (0.042)\tLoss 0.3403 (0.3939)\tPrec@1 88.281 (85.826)\n",
      "Test: [170/313]\tTime 0.041 (0.042)\tLoss 0.2924 (0.3928)\tPrec@1 91.406 (85.846)\n",
      "Test: [180/313]\tTime 0.041 (0.042)\tLoss 0.2849 (0.3922)\tPrec@1 89.844 (85.843)\n",
      "Test: [190/313]\tTime 0.042 (0.042)\tLoss 0.4687 (0.3926)\tPrec@1 79.688 (85.794)\n",
      "Test: [200/313]\tTime 0.041 (0.042)\tLoss 0.5112 (0.3933)\tPrec@1 82.812 (85.809)\n",
      "Test: [210/313]\tTime 0.041 (0.042)\tLoss 0.3629 (0.3957)\tPrec@1 85.938 (85.763)\n",
      "Test: [220/313]\tTime 0.041 (0.042)\tLoss 0.4410 (0.3973)\tPrec@1 82.812 (85.725)\n",
      "Test: [230/313]\tTime 0.041 (0.042)\tLoss 0.3670 (0.3982)\tPrec@1 85.938 (85.677)\n",
      "Test: [240/313]\tTime 0.041 (0.042)\tLoss 0.3895 (0.3972)\tPrec@1 88.281 (85.701)\n",
      "Test: [250/313]\tTime 0.041 (0.042)\tLoss 0.4389 (0.3992)\tPrec@1 86.719 (85.639)\n",
      "Test: [260/313]\tTime 0.041 (0.042)\tLoss 0.5940 (0.4008)\tPrec@1 83.594 (85.617)\n",
      "Test: [270/313]\tTime 0.041 (0.042)\tLoss 0.4664 (0.4022)\tPrec@1 81.250 (85.566)\n",
      "Test: [280/313]\tTime 0.041 (0.042)\tLoss 0.4377 (0.4025)\tPrec@1 82.031 (85.587)\n",
      "Test: [290/313]\tTime 0.041 (0.042)\tLoss 0.4717 (0.4037)\tPrec@1 81.250 (85.570)\n",
      "Test: [300/313]\tTime 0.041 (0.042)\tLoss 0.4156 (0.4033)\tPrec@1 82.031 (85.590)\n",
      "Test: [310/313]\tTime 0.041 (0.042)\tLoss 0.3003 (0.4026)\tPrec@1 90.625 (85.603)\n",
      " * Prec@1 85.582\n",
      "Epoch: [3][0/235]\tTime 0.250 (0.250)\tLoss 0.2111 (0.2111)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [3][10/235]\tTime 0.145 (0.154)\tLoss 0.2463 (0.2353)\tPrec@1 91.406 (90.980)\n",
      "Epoch: [3][20/235]\tTime 0.144 (0.149)\tLoss 0.1739 (0.2349)\tPrec@1 96.094 (91.220)\n",
      "Epoch: [3][30/235]\tTime 0.144 (0.148)\tLoss 0.1488 (0.2349)\tPrec@1 94.531 (91.406)\n",
      "Epoch: [3][40/235]\tTime 0.143 (0.147)\tLoss 0.1997 (0.2358)\tPrec@1 92.969 (91.254)\n",
      "Epoch: [3][50/235]\tTime 0.144 (0.146)\tLoss 0.2140 (0.2303)\tPrec@1 91.406 (91.437)\n",
      "Epoch: [3][60/235]\tTime 0.144 (0.146)\tLoss 0.1718 (0.2266)\tPrec@1 91.406 (91.534)\n",
      "Epoch: [3][70/235]\tTime 0.144 (0.146)\tLoss 0.1982 (0.2293)\tPrec@1 93.750 (91.505)\n",
      "Epoch: [3][80/235]\tTime 0.144 (0.145)\tLoss 0.2864 (0.2347)\tPrec@1 89.844 (91.300)\n",
      "Epoch: [3][90/235]\tTime 0.144 (0.145)\tLoss 0.2463 (0.2338)\tPrec@1 87.500 (91.346)\n",
      "Epoch: [3][100/235]\tTime 0.144 (0.145)\tLoss 0.2019 (0.2336)\tPrec@1 92.969 (91.344)\n",
      "Epoch: [3][110/235]\tTime 0.144 (0.145)\tLoss 0.1665 (0.2336)\tPrec@1 94.531 (91.357)\n",
      "Epoch: [3][120/235]\tTime 0.144 (0.145)\tLoss 0.2438 (0.2327)\tPrec@1 91.406 (91.374)\n",
      "Epoch: [3][130/235]\tTime 0.143 (0.145)\tLoss 0.1205 (0.2318)\tPrec@1 94.531 (91.472)\n",
      "Epoch: [3][140/235]\tTime 0.144 (0.145)\tLoss 0.2288 (0.2354)\tPrec@1 90.625 (91.417)\n",
      "Epoch: [3][150/235]\tTime 0.143 (0.145)\tLoss 0.3709 (0.2367)\tPrec@1 88.281 (91.386)\n",
      "Epoch: [3][160/235]\tTime 0.144 (0.145)\tLoss 0.3430 (0.2389)\tPrec@1 86.719 (91.280)\n",
      "Epoch: [3][170/235]\tTime 0.145 (0.145)\tLoss 0.3119 (0.2402)\tPrec@1 89.844 (91.246)\n",
      "Epoch: [3][180/235]\tTime 0.144 (0.145)\tLoss 0.2651 (0.2401)\tPrec@1 92.188 (91.294)\n",
      "Epoch: [3][190/235]\tTime 0.145 (0.145)\tLoss 0.2129 (0.2391)\tPrec@1 93.750 (91.320)\n",
      "Epoch: [3][200/235]\tTime 0.143 (0.145)\tLoss 0.2794 (0.2374)\tPrec@1 87.500 (91.387)\n",
      "Epoch: [3][210/235]\tTime 0.143 (0.145)\tLoss 0.2844 (0.2368)\tPrec@1 87.500 (91.403)\n",
      "Epoch: [3][220/235]\tTime 0.145 (0.145)\tLoss 0.3009 (0.2356)\tPrec@1 89.844 (91.442)\n",
      "Epoch: [3][230/235]\tTime 0.143 (0.145)\tLoss 0.1763 (0.2341)\tPrec@1 93.750 (91.494)\n",
      "Test: [0/313]\tTime 0.205 (0.205)\tLoss 0.1606 (0.1606)\tPrec@1 93.750 (93.750)\n",
      "Test: [10/313]\tTime 0.041 (0.056)\tLoss 0.2439 (0.2657)\tPrec@1 89.062 (90.909)\n",
      "Test: [20/313]\tTime 0.041 (0.049)\tLoss 0.4343 (0.2770)\tPrec@1 86.719 (90.253)\n",
      "Test: [30/313]\tTime 0.041 (0.047)\tLoss 0.2497 (0.2707)\tPrec@1 93.750 (90.701)\n",
      "Test: [40/313]\tTime 0.041 (0.045)\tLoss 0.2221 (0.2655)\tPrec@1 92.969 (90.777)\n",
      "Test: [50/313]\tTime 0.041 (0.045)\tLoss 0.2178 (0.2660)\tPrec@1 92.969 (90.594)\n",
      "Test: [60/313]\tTime 0.041 (0.044)\tLoss 0.2698 (0.2629)\tPrec@1 89.844 (90.663)\n",
      "Test: [70/313]\tTime 0.041 (0.044)\tLoss 0.3114 (0.2610)\tPrec@1 87.500 (90.680)\n",
      "Test: [80/313]\tTime 0.041 (0.043)\tLoss 0.2417 (0.2599)\tPrec@1 91.406 (90.712)\n",
      "Test: [90/313]\tTime 0.041 (0.043)\tLoss 0.1959 (0.2592)\tPrec@1 92.969 (90.711)\n",
      "Test: [100/313]\tTime 0.041 (0.043)\tLoss 0.1975 (0.2618)\tPrec@1 93.750 (90.633)\n",
      "Test: [110/313]\tTime 0.041 (0.043)\tLoss 0.2366 (0.2608)\tPrec@1 87.500 (90.695)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [120/313]\tTime 0.041 (0.043)\tLoss 0.3550 (0.2627)\tPrec@1 83.594 (90.567)\n",
      "Test: [130/313]\tTime 0.041 (0.043)\tLoss 0.2340 (0.2628)\tPrec@1 92.188 (90.571)\n",
      "Test: [140/313]\tTime 0.041 (0.043)\tLoss 0.3847 (0.2632)\tPrec@1 90.625 (90.603)\n",
      "Test: [150/313]\tTime 0.041 (0.042)\tLoss 0.2012 (0.2662)\tPrec@1 91.406 (90.465)\n",
      "Test: [160/313]\tTime 0.041 (0.042)\tLoss 0.2437 (0.2659)\tPrec@1 92.969 (90.431)\n",
      "Test: [170/313]\tTime 0.041 (0.042)\tLoss 0.2800 (0.2661)\tPrec@1 89.844 (90.470)\n",
      "Test: [180/313]\tTime 0.041 (0.042)\tLoss 0.2192 (0.2670)\tPrec@1 89.062 (90.418)\n",
      "Test: [190/313]\tTime 0.041 (0.042)\tLoss 0.3208 (0.2662)\tPrec@1 91.406 (90.502)\n",
      "Test: [200/313]\tTime 0.041 (0.042)\tLoss 0.2940 (0.2674)\tPrec@1 91.406 (90.442)\n",
      "Test: [210/313]\tTime 0.041 (0.042)\tLoss 0.2837 (0.2689)\tPrec@1 92.188 (90.421)\n",
      "Test: [220/313]\tTime 0.041 (0.042)\tLoss 0.3064 (0.2678)\tPrec@1 87.500 (90.480)\n",
      "Test: [230/313]\tTime 0.041 (0.042)\tLoss 0.2268 (0.2675)\tPrec@1 91.406 (90.483)\n",
      "Test: [240/313]\tTime 0.041 (0.042)\tLoss 0.3781 (0.2682)\tPrec@1 87.500 (90.463)\n",
      "Test: [250/313]\tTime 0.041 (0.042)\tLoss 0.2084 (0.2668)\tPrec@1 90.625 (90.525)\n",
      "Test: [260/313]\tTime 0.041 (0.042)\tLoss 0.2519 (0.2654)\tPrec@1 88.281 (90.577)\n",
      "Test: [270/313]\tTime 0.041 (0.042)\tLoss 0.2078 (0.2650)\tPrec@1 92.969 (90.599)\n",
      "Test: [280/313]\tTime 0.041 (0.042)\tLoss 0.2361 (0.2639)\tPrec@1 91.406 (90.639)\n",
      "Test: [290/313]\tTime 0.041 (0.042)\tLoss 0.4379 (0.2641)\tPrec@1 88.281 (90.652)\n",
      "Test: [300/313]\tTime 0.041 (0.042)\tLoss 0.3287 (0.2627)\tPrec@1 92.188 (90.718)\n",
      "Test: [310/313]\tTime 0.041 (0.042)\tLoss 0.2992 (0.2617)\tPrec@1 88.281 (90.741)\n",
      " * Prec@1 90.745\n",
      "Epoch: [4][0/235]\tTime 0.247 (0.247)\tLoss 0.2078 (0.2078)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [4][10/235]\tTime 0.144 (0.154)\tLoss 0.2158 (0.1913)\tPrec@1 91.406 (93.182)\n",
      "Epoch: [4][20/235]\tTime 0.144 (0.149)\tLoss 0.1890 (0.1978)\tPrec@1 92.969 (93.043)\n",
      "Epoch: [4][30/235]\tTime 0.145 (0.148)\tLoss 0.1705 (0.2061)\tPrec@1 94.531 (92.666)\n",
      "Epoch: [4][40/235]\tTime 0.144 (0.147)\tLoss 0.1837 (0.2003)\tPrec@1 90.625 (92.797)\n",
      "Epoch: [4][50/235]\tTime 0.144 (0.146)\tLoss 0.2014 (0.2000)\tPrec@1 92.969 (92.862)\n",
      "Epoch: [4][60/235]\tTime 0.144 (0.146)\tLoss 0.1268 (0.2002)\tPrec@1 94.531 (92.764)\n",
      "Epoch: [4][70/235]\tTime 0.144 (0.146)\tLoss 0.1778 (0.2018)\tPrec@1 92.969 (92.694)\n",
      "Epoch: [4][80/235]\tTime 0.147 (0.145)\tLoss 0.2851 (0.1992)\tPrec@1 89.062 (92.708)\n",
      "Epoch: [4][90/235]\tTime 0.144 (0.145)\tLoss 0.1734 (0.2012)\tPrec@1 94.531 (92.728)\n",
      "Epoch: [4][100/235]\tTime 0.144 (0.145)\tLoss 0.2519 (0.2040)\tPrec@1 86.719 (92.574)\n",
      "Epoch: [4][110/235]\tTime 0.144 (0.145)\tLoss 0.1958 (0.2063)\tPrec@1 94.531 (92.518)\n",
      "Epoch: [4][120/235]\tTime 0.143 (0.145)\tLoss 0.1966 (0.2070)\tPrec@1 92.969 (92.543)\n",
      "Epoch: [4][130/235]\tTime 0.144 (0.145)\tLoss 0.1479 (0.2062)\tPrec@1 94.531 (92.569)\n",
      "Epoch: [4][140/235]\tTime 0.144 (0.145)\tLoss 0.2598 (0.2056)\tPrec@1 90.625 (92.603)\n",
      "Epoch: [4][150/235]\tTime 0.146 (0.145)\tLoss 0.2064 (0.2045)\tPrec@1 89.062 (92.586)\n",
      "Epoch: [4][160/235]\tTime 0.144 (0.145)\tLoss 0.1973 (0.2048)\tPrec@1 89.844 (92.566)\n",
      "Epoch: [4][170/235]\tTime 0.144 (0.145)\tLoss 0.2622 (0.2068)\tPrec@1 91.406 (92.498)\n",
      "Epoch: [4][180/235]\tTime 0.144 (0.145)\tLoss 0.1821 (0.2063)\tPrec@1 92.969 (92.490)\n",
      "Epoch: [4][190/235]\tTime 0.144 (0.145)\tLoss 0.1571 (0.2073)\tPrec@1 93.750 (92.490)\n",
      "Epoch: [4][200/235]\tTime 0.144 (0.145)\tLoss 0.2692 (0.2074)\tPrec@1 89.844 (92.491)\n",
      "Epoch: [4][210/235]\tTime 0.145 (0.145)\tLoss 0.2493 (0.2072)\tPrec@1 89.844 (92.499)\n",
      "Epoch: [4][220/235]\tTime 0.144 (0.145)\tLoss 0.1555 (0.2063)\tPrec@1 95.312 (92.516)\n",
      "Epoch: [4][230/235]\tTime 0.144 (0.145)\tLoss 0.2296 (0.2067)\tPrec@1 90.625 (92.489)\n",
      "Test: [0/313]\tTime 0.198 (0.198)\tLoss 0.4244 (0.4244)\tPrec@1 87.500 (87.500)\n",
      "Test: [10/313]\tTime 0.041 (0.056)\tLoss 0.3211 (0.3074)\tPrec@1 85.938 (88.636)\n",
      "Test: [20/313]\tTime 0.041 (0.049)\tLoss 0.2620 (0.2950)\tPrec@1 90.625 (89.435)\n",
      "Test: [30/313]\tTime 0.041 (0.047)\tLoss 0.2736 (0.2981)\tPrec@1 92.188 (89.667)\n",
      "Test: [40/313]\tTime 0.041 (0.045)\tLoss 0.2366 (0.2878)\tPrec@1 93.750 (90.091)\n",
      "Test: [50/313]\tTime 0.042 (0.045)\tLoss 0.2330 (0.2790)\tPrec@1 91.406 (90.288)\n",
      "Test: [60/313]\tTime 0.041 (0.044)\tLoss 0.2946 (0.2730)\tPrec@1 86.719 (90.484)\n",
      "Test: [70/313]\tTime 0.041 (0.044)\tLoss 0.2864 (0.2731)\tPrec@1 89.062 (90.416)\n",
      "Test: [80/313]\tTime 0.042 (0.043)\tLoss 0.2175 (0.2735)\tPrec@1 92.969 (90.316)\n",
      "Test: [90/313]\tTime 0.041 (0.043)\tLoss 0.2478 (0.2750)\tPrec@1 90.625 (90.204)\n",
      "Test: [100/313]\tTime 0.041 (0.043)\tLoss 0.2478 (0.2764)\tPrec@1 92.188 (90.176)\n",
      "Test: [110/313]\tTime 0.041 (0.043)\tLoss 0.2070 (0.2756)\tPrec@1 92.969 (90.196)\n",
      "Test: [120/313]\tTime 0.041 (0.043)\tLoss 0.2421 (0.2751)\tPrec@1 93.750 (90.225)\n",
      "Test: [130/313]\tTime 0.041 (0.043)\tLoss 0.2818 (0.2743)\tPrec@1 89.062 (90.261)\n",
      "Test: [140/313]\tTime 0.041 (0.043)\tLoss 0.3430 (0.2740)\tPrec@1 85.938 (90.221)\n",
      "Test: [150/313]\tTime 0.041 (0.042)\tLoss 0.1119 (0.2725)\tPrec@1 96.094 (90.242)\n",
      "Test: [160/313]\tTime 0.041 (0.042)\tLoss 0.1492 (0.2723)\tPrec@1 95.312 (90.261)\n",
      "Test: [170/313]\tTime 0.041 (0.042)\tLoss 0.4309 (0.2738)\tPrec@1 84.375 (90.196)\n",
      "Test: [180/313]\tTime 0.041 (0.042)\tLoss 0.4318 (0.2733)\tPrec@1 86.719 (90.228)\n",
      "Test: [190/313]\tTime 0.041 (0.042)\tLoss 0.2444 (0.2732)\tPrec@1 89.844 (90.212)\n",
      "Test: [200/313]\tTime 0.041 (0.042)\tLoss 0.2094 (0.2740)\tPrec@1 91.406 (90.205)\n",
      "Test: [210/313]\tTime 0.042 (0.042)\tLoss 0.2561 (0.2747)\tPrec@1 92.188 (90.192)\n",
      "Test: [220/313]\tTime 0.041 (0.042)\tLoss 0.2794 (0.2745)\tPrec@1 91.406 (90.229)\n",
      "Test: [230/313]\tTime 0.041 (0.042)\tLoss 0.3414 (0.2759)\tPrec@1 88.281 (90.168)\n",
      "Test: [240/313]\tTime 0.042 (0.042)\tLoss 0.3964 (0.2765)\tPrec@1 87.500 (90.181)\n",
      "Test: [250/313]\tTime 0.041 (0.042)\tLoss 0.3307 (0.2758)\tPrec@1 90.625 (90.202)\n",
      "Test: [260/313]\tTime 0.041 (0.042)\tLoss 0.1596 (0.2760)\tPrec@1 93.750 (90.230)\n",
      "Test: [270/313]\tTime 0.041 (0.042)\tLoss 0.3074 (0.2762)\tPrec@1 89.062 (90.207)\n",
      "Test: [280/313]\tTime 0.041 (0.042)\tLoss 0.2411 (0.2766)\tPrec@1 93.750 (90.200)\n",
      "Test: [290/313]\tTime 0.041 (0.042)\tLoss 0.2042 (0.2774)\tPrec@1 91.406 (90.134)\n",
      "Test: [300/313]\tTime 0.042 (0.042)\tLoss 0.2824 (0.2771)\tPrec@1 89.844 (90.150)\n",
      "Test: [310/313]\tTime 0.041 (0.042)\tLoss 0.4086 (0.2768)\tPrec@1 85.938 (90.163)\n",
      " * Prec@1 90.185\n",
      "Epoch: [5][0/235]\tTime 0.298 (0.298)\tLoss 0.1880 (0.1880)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [5][10/235]\tTime 0.145 (0.158)\tLoss 0.1865 (0.1873)\tPrec@1 94.531 (92.614)\n",
      "Epoch: [5][20/235]\tTime 0.146 (0.152)\tLoss 0.1696 (0.1830)\tPrec@1 94.531 (93.043)\n",
      "Epoch: [5][30/235]\tTime 0.144 (0.149)\tLoss 0.1380 (0.1846)\tPrec@1 94.531 (92.944)\n",
      "Epoch: [5][40/235]\tTime 0.144 (0.148)\tLoss 0.1701 (0.1839)\tPrec@1 93.750 (93.083)\n",
      "Epoch: [5][50/235]\tTime 0.145 (0.147)\tLoss 0.1490 (0.1861)\tPrec@1 96.875 (93.244)\n",
      "Epoch: [5][60/235]\tTime 0.143 (0.147)\tLoss 0.1820 (0.1859)\tPrec@1 95.312 (93.199)\n",
      "Epoch: [5][70/235]\tTime 0.144 (0.147)\tLoss 0.2164 (0.1896)\tPrec@1 91.406 (93.079)\n",
      "Epoch: [5][80/235]\tTime 0.143 (0.146)\tLoss 0.2364 (0.1872)\tPrec@1 92.969 (93.181)\n",
      "Epoch: [5][90/235]\tTime 0.145 (0.146)\tLoss 0.2860 (0.1895)\tPrec@1 88.281 (93.037)\n",
      "Epoch: [5][100/235]\tTime 0.145 (0.146)\tLoss 0.2233 (0.1881)\tPrec@1 92.188 (93.077)\n",
      "Epoch: [5][110/235]\tTime 0.144 (0.146)\tLoss 0.1847 (0.1877)\tPrec@1 93.750 (93.117)\n",
      "Epoch: [5][120/235]\tTime 0.146 (0.146)\tLoss 0.1927 (0.1873)\tPrec@1 91.406 (93.137)\n",
      "Epoch: [5][130/235]\tTime 0.144 (0.146)\tLoss 0.2308 (0.1877)\tPrec@1 90.625 (93.148)\n",
      "Epoch: [5][140/235]\tTime 0.145 (0.146)\tLoss 0.3222 (0.1879)\tPrec@1 88.281 (93.118)\n",
      "Epoch: [5][150/235]\tTime 0.146 (0.146)\tLoss 0.1527 (0.1879)\tPrec@1 93.750 (93.134)\n",
      "Epoch: [5][160/235]\tTime 0.144 (0.145)\tLoss 0.1299 (0.1874)\tPrec@1 96.875 (93.187)\n",
      "Epoch: [5][170/235]\tTime 0.144 (0.145)\tLoss 0.1901 (0.1866)\tPrec@1 93.750 (93.229)\n",
      "Epoch: [5][180/235]\tTime 0.144 (0.145)\tLoss 0.1977 (0.1881)\tPrec@1 92.969 (93.167)\n",
      "Epoch: [5][190/235]\tTime 0.146 (0.145)\tLoss 0.2685 (0.1887)\tPrec@1 91.406 (93.108)\n",
      "Epoch: [5][200/235]\tTime 0.144 (0.145)\tLoss 0.1783 (0.1897)\tPrec@1 93.750 (93.058)\n",
      "Epoch: [5][210/235]\tTime 0.144 (0.145)\tLoss 0.2375 (0.1886)\tPrec@1 91.406 (93.069)\n",
      "Epoch: [5][220/235]\tTime 0.144 (0.145)\tLoss 0.1331 (0.1881)\tPrec@1 95.312 (93.075)\n",
      "Epoch: [5][230/235]\tTime 0.144 (0.145)\tLoss 0.2566 (0.1875)\tPrec@1 92.969 (93.084)\n",
      "Test: [0/313]\tTime 0.211 (0.211)\tLoss 0.2733 (0.2733)\tPrec@1 89.844 (89.844)\n",
      "Test: [10/313]\tTime 0.041 (0.057)\tLoss 0.2772 (0.2609)\tPrec@1 91.406 (91.406)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [20/313]\tTime 0.041 (0.050)\tLoss 0.2636 (0.2445)\tPrec@1 88.281 (91.295)\n",
      "Test: [30/313]\tTime 0.041 (0.047)\tLoss 0.3491 (0.2445)\tPrec@1 89.062 (91.482)\n",
      "Test: [40/313]\tTime 0.041 (0.046)\tLoss 0.2413 (0.2453)\tPrec@1 90.625 (91.425)\n",
      "Test: [50/313]\tTime 0.041 (0.045)\tLoss 0.2463 (0.2409)\tPrec@1 92.188 (91.820)\n",
      "Test: [60/313]\tTime 0.041 (0.044)\tLoss 0.1851 (0.2407)\tPrec@1 92.188 (91.829)\n",
      "Test: [70/313]\tTime 0.041 (0.044)\tLoss 0.2236 (0.2421)\tPrec@1 92.969 (91.824)\n",
      "Test: [80/313]\tTime 0.041 (0.044)\tLoss 0.3414 (0.2418)\tPrec@1 89.062 (91.792)\n",
      "Test: [90/313]\tTime 0.041 (0.043)\tLoss 0.1815 (0.2375)\tPrec@1 92.188 (91.870)\n",
      "Test: [100/313]\tTime 0.041 (0.043)\tLoss 0.1967 (0.2401)\tPrec@1 92.969 (91.808)\n",
      "Test: [110/313]\tTime 0.041 (0.043)\tLoss 0.2162 (0.2387)\tPrec@1 94.531 (91.829)\n",
      "Test: [120/313]\tTime 0.041 (0.043)\tLoss 0.2545 (0.2380)\tPrec@1 90.625 (91.871)\n",
      "Test: [130/313]\tTime 0.041 (0.043)\tLoss 0.3240 (0.2379)\tPrec@1 87.500 (91.854)\n",
      "Test: [140/313]\tTime 0.041 (0.043)\tLoss 0.2184 (0.2369)\tPrec@1 92.188 (91.872)\n",
      "Test: [150/313]\tTime 0.042 (0.043)\tLoss 0.1758 (0.2370)\tPrec@1 93.750 (91.841)\n",
      "Test: [160/313]\tTime 0.041 (0.042)\tLoss 0.2404 (0.2365)\tPrec@1 88.281 (91.799)\n",
      "Test: [170/313]\tTime 0.041 (0.042)\tLoss 0.2682 (0.2372)\tPrec@1 94.531 (91.808)\n",
      "Test: [180/313]\tTime 0.041 (0.042)\tLoss 0.1646 (0.2360)\tPrec@1 92.969 (91.821)\n",
      "Test: [190/313]\tTime 0.041 (0.042)\tLoss 0.1994 (0.2357)\tPrec@1 92.188 (91.799)\n",
      "Test: [200/313]\tTime 0.041 (0.042)\tLoss 0.2826 (0.2347)\tPrec@1 85.938 (91.830)\n",
      "Test: [210/313]\tTime 0.041 (0.042)\tLoss 0.1960 (0.2345)\tPrec@1 92.969 (91.814)\n",
      "Test: [220/313]\tTime 0.041 (0.042)\tLoss 0.2406 (0.2351)\tPrec@1 89.844 (91.767)\n",
      "Test: [230/313]\tTime 0.041 (0.042)\tLoss 0.1723 (0.2359)\tPrec@1 92.969 (91.734)\n",
      "Test: [240/313]\tTime 0.041 (0.042)\tLoss 0.2437 (0.2358)\tPrec@1 90.625 (91.721)\n",
      "Test: [250/313]\tTime 0.042 (0.042)\tLoss 0.2083 (0.2364)\tPrec@1 90.625 (91.665)\n",
      "Test: [260/313]\tTime 0.041 (0.042)\tLoss 0.1340 (0.2361)\tPrec@1 95.312 (91.673)\n",
      "Test: [270/313]\tTime 0.041 (0.042)\tLoss 0.3482 (0.2353)\tPrec@1 88.281 (91.700)\n",
      "Test: [280/313]\tTime 0.041 (0.042)\tLoss 0.4664 (0.2352)\tPrec@1 85.156 (91.704)\n",
      "Test: [290/313]\tTime 0.041 (0.042)\tLoss 0.2343 (0.2351)\tPrec@1 93.750 (91.712)\n",
      "Test: [300/313]\tTime 0.041 (0.042)\tLoss 0.3304 (0.2347)\tPrec@1 89.062 (91.738)\n",
      "Test: [310/313]\tTime 0.041 (0.042)\tLoss 0.2309 (0.2353)\tPrec@1 92.188 (91.750)\n",
      " * Prec@1 91.760\n",
      "Epoch: [6][0/235]\tTime 0.268 (0.268)\tLoss 0.0933 (0.0933)\tPrec@1 97.656 (97.656)\n",
      "Epoch: [6][10/235]\tTime 0.144 (0.156)\tLoss 0.0727 (0.1588)\tPrec@1 97.656 (94.318)\n",
      "Epoch: [6][20/235]\tTime 0.144 (0.150)\tLoss 0.1858 (0.1517)\tPrec@1 92.969 (94.568)\n",
      "Epoch: [6][30/235]\tTime 0.145 (0.148)\tLoss 0.1253 (0.1433)\tPrec@1 93.750 (94.783)\n",
      "Epoch: [6][40/235]\tTime 0.144 (0.147)\tLoss 0.1700 (0.1519)\tPrec@1 92.969 (94.455)\n",
      "Epoch: [6][50/235]\tTime 0.145 (0.147)\tLoss 0.1221 (0.1528)\tPrec@1 96.094 (94.424)\n",
      "Epoch: [6][60/235]\tTime 0.146 (0.147)\tLoss 0.0851 (0.1489)\tPrec@1 96.875 (94.595)\n",
      "Epoch: [6][70/235]\tTime 0.144 (0.146)\tLoss 0.1996 (0.1556)\tPrec@1 91.406 (94.300)\n",
      "Epoch: [6][80/235]\tTime 0.144 (0.146)\tLoss 0.1350 (0.1577)\tPrec@1 96.094 (94.252)\n",
      "Epoch: [6][90/235]\tTime 0.144 (0.146)\tLoss 0.2707 (0.1604)\tPrec@1 91.406 (94.154)\n",
      "Epoch: [6][100/235]\tTime 0.144 (0.146)\tLoss 0.1337 (0.1619)\tPrec@1 96.875 (94.114)\n",
      "Epoch: [6][110/235]\tTime 0.144 (0.146)\tLoss 0.2872 (0.1640)\tPrec@1 87.500 (93.961)\n",
      "Epoch: [6][120/235]\tTime 0.144 (0.146)\tLoss 0.1813 (0.1646)\tPrec@1 92.969 (93.937)\n",
      "Epoch: [6][130/235]\tTime 0.145 (0.145)\tLoss 0.1823 (0.1652)\tPrec@1 95.312 (93.941)\n",
      "Epoch: [6][140/235]\tTime 0.144 (0.145)\tLoss 0.1454 (0.1649)\tPrec@1 93.750 (93.955)\n",
      "Epoch: [6][150/235]\tTime 0.145 (0.145)\tLoss 0.1108 (0.1642)\tPrec@1 95.312 (93.978)\n",
      "Epoch: [6][160/235]\tTime 0.144 (0.145)\tLoss 0.1165 (0.1650)\tPrec@1 94.531 (93.944)\n",
      "Epoch: [6][170/235]\tTime 0.144 (0.145)\tLoss 0.2365 (0.1663)\tPrec@1 90.625 (93.924)\n",
      "Epoch: [6][180/235]\tTime 0.145 (0.145)\tLoss 0.1400 (0.1677)\tPrec@1 95.312 (93.862)\n",
      "Epoch: [6][190/235]\tTime 0.144 (0.145)\tLoss 0.1845 (0.1678)\tPrec@1 92.188 (93.832)\n",
      "Epoch: [6][200/235]\tTime 0.144 (0.145)\tLoss 0.0735 (0.1678)\tPrec@1 96.875 (93.836)\n",
      "Epoch: [6][210/235]\tTime 0.144 (0.145)\tLoss 0.1506 (0.1688)\tPrec@1 92.969 (93.794)\n",
      "Epoch: [6][220/235]\tTime 0.144 (0.145)\tLoss 0.1925 (0.1690)\tPrec@1 91.406 (93.771)\n",
      "Epoch: [6][230/235]\tTime 0.143 (0.145)\tLoss 0.2809 (0.1707)\tPrec@1 91.406 (93.733)\n",
      "Test: [0/313]\tTime 0.207 (0.207)\tLoss 0.2552 (0.2552)\tPrec@1 89.062 (89.062)\n",
      "Test: [10/313]\tTime 0.042 (0.056)\tLoss 0.4682 (0.3034)\tPrec@1 88.281 (89.276)\n",
      "Test: [20/313]\tTime 0.041 (0.049)\tLoss 0.2307 (0.3055)\tPrec@1 89.844 (89.249)\n",
      "Test: [30/313]\tTime 0.041 (0.047)\tLoss 0.2601 (0.2965)\tPrec@1 91.406 (89.289)\n",
      "Test: [40/313]\tTime 0.041 (0.045)\tLoss 0.1490 (0.2995)\tPrec@1 95.312 (89.272)\n",
      "Test: [50/313]\tTime 0.041 (0.045)\tLoss 0.2796 (0.3005)\tPrec@1 89.062 (89.338)\n",
      "Test: [60/313]\tTime 0.041 (0.044)\tLoss 0.4124 (0.3024)\tPrec@1 89.062 (89.383)\n",
      "Test: [70/313]\tTime 0.041 (0.044)\tLoss 0.2777 (0.2957)\tPrec@1 89.062 (89.624)\n",
      "Test: [80/313]\tTime 0.041 (0.043)\tLoss 0.1612 (0.3016)\tPrec@1 96.875 (89.564)\n",
      "Test: [90/313]\tTime 0.041 (0.043)\tLoss 0.3737 (0.3067)\tPrec@1 87.500 (89.466)\n",
      "Test: [100/313]\tTime 0.041 (0.043)\tLoss 0.3800 (0.3054)\tPrec@1 86.719 (89.411)\n",
      "Test: [110/313]\tTime 0.041 (0.043)\tLoss 0.3455 (0.3038)\tPrec@1 86.719 (89.443)\n",
      "Test: [120/313]\tTime 0.041 (0.043)\tLoss 0.2111 (0.3028)\tPrec@1 92.969 (89.443)\n",
      "Test: [130/313]\tTime 0.041 (0.043)\tLoss 0.3417 (0.3036)\tPrec@1 88.281 (89.414)\n",
      "Test: [140/313]\tTime 0.041 (0.043)\tLoss 0.2505 (0.3034)\tPrec@1 92.969 (89.400)\n",
      "Test: [150/313]\tTime 0.041 (0.042)\tLoss 0.3669 (0.3073)\tPrec@1 88.281 (89.275)\n",
      "Test: [160/313]\tTime 0.041 (0.042)\tLoss 0.2441 (0.3046)\tPrec@1 90.625 (89.349)\n",
      "Test: [170/313]\tTime 0.041 (0.042)\tLoss 0.3822 (0.3036)\tPrec@1 92.969 (89.382)\n",
      "Test: [180/313]\tTime 0.041 (0.042)\tLoss 0.2640 (0.3029)\tPrec@1 91.406 (89.425)\n",
      "Test: [190/313]\tTime 0.041 (0.042)\tLoss 0.4116 (0.3033)\tPrec@1 87.500 (89.406)\n",
      "Test: [200/313]\tTime 0.041 (0.042)\tLoss 0.2964 (0.3033)\tPrec@1 90.625 (89.428)\n",
      "Test: [210/313]\tTime 0.041 (0.042)\tLoss 0.3316 (0.3042)\tPrec@1 88.281 (89.407)\n",
      "Test: [220/313]\tTime 0.041 (0.042)\tLoss 0.3442 (0.3049)\tPrec@1 89.844 (89.391)\n",
      "Test: [230/313]\tTime 0.041 (0.042)\tLoss 0.1575 (0.3043)\tPrec@1 92.969 (89.428)\n",
      "Test: [240/313]\tTime 0.041 (0.042)\tLoss 0.3001 (0.3049)\tPrec@1 89.062 (89.400)\n",
      "Test: [250/313]\tTime 0.041 (0.042)\tLoss 0.1509 (0.3046)\tPrec@1 95.312 (89.411)\n",
      "Test: [260/313]\tTime 0.042 (0.042)\tLoss 0.3840 (0.3058)\tPrec@1 89.062 (89.389)\n",
      "Test: [270/313]\tTime 0.041 (0.042)\tLoss 0.4028 (0.3064)\tPrec@1 86.719 (89.365)\n",
      "Test: [280/313]\tTime 0.042 (0.042)\tLoss 0.4275 (0.3065)\tPrec@1 87.500 (89.377)\n",
      "Test: [290/313]\tTime 0.041 (0.042)\tLoss 0.3445 (0.3066)\tPrec@1 87.500 (89.385)\n",
      "Test: [300/313]\tTime 0.041 (0.042)\tLoss 0.2156 (0.3082)\tPrec@1 93.750 (89.369)\n",
      "Test: [310/313]\tTime 0.041 (0.042)\tLoss 0.3987 (0.3078)\tPrec@1 88.281 (89.392)\n",
      " * Prec@1 89.392\n",
      "Epoch: [7][0/235]\tTime 0.304 (0.304)\tLoss 0.1293 (0.1293)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [7][10/235]\tTime 0.145 (0.159)\tLoss 0.0891 (0.1542)\tPrec@1 98.438 (94.744)\n",
      "Epoch: [7][20/235]\tTime 0.145 (0.152)\tLoss 0.1240 (0.1610)\tPrec@1 96.094 (94.531)\n",
      "Epoch: [7][30/235]\tTime 0.145 (0.149)\tLoss 0.2030 (0.1534)\tPrec@1 91.406 (94.708)\n",
      "Epoch: [7][40/235]\tTime 0.143 (0.148)\tLoss 0.1304 (0.1491)\tPrec@1 93.750 (94.874)\n",
      "Epoch: [7][50/235]\tTime 0.144 (0.147)\tLoss 0.1123 (0.1496)\tPrec@1 96.094 (94.822)\n",
      "Epoch: [7][60/235]\tTime 0.145 (0.147)\tLoss 0.0836 (0.1469)\tPrec@1 98.438 (94.915)\n",
      "Epoch: [7][70/235]\tTime 0.143 (0.146)\tLoss 0.1356 (0.1466)\tPrec@1 95.312 (94.916)\n",
      "Epoch: [7][80/235]\tTime 0.144 (0.146)\tLoss 0.1031 (0.1485)\tPrec@1 96.875 (94.840)\n",
      "Epoch: [7][90/235]\tTime 0.144 (0.146)\tLoss 0.2464 (0.1491)\tPrec@1 92.188 (94.857)\n",
      "Epoch: [7][100/235]\tTime 0.144 (0.146)\tLoss 0.2292 (0.1507)\tPrec@1 92.188 (94.771)\n",
      "Epoch: [7][110/235]\tTime 0.144 (0.146)\tLoss 0.0473 (0.1511)\tPrec@1 98.438 (94.764)\n",
      "Epoch: [7][120/235]\tTime 0.145 (0.145)\tLoss 0.2007 (0.1523)\tPrec@1 89.844 (94.615)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(hparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Accuracy: 93.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
